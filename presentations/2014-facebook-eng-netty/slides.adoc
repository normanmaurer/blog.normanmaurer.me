= _Netty_ Best Practices a.k.a __Faster == Better__
@Facebook 2014 ; Palo Alto ; 2014/03/19
include::attributes.adoc[]

:experimental:
:toc2:
:sectanchors:
:idprefix:
:idseparator: -
:icons: font
:source-highlighter: coderay

[.topic.source]
== Norman Maurer, Principal Software Engineer @ Red Hat Inc

====
* [icon-user]'{zwsp}' __Netty__ / Vert.x / All things NIO
* [icon-user]'{zwsp}' Author of __Netty in Action__
* [icon-twitter]'{zwsp}' @normanmaurer
* [icon-github]'{zwsp}' github.com/normanmaurer
====

[.topic.source]
== Agenda

====
* [icon-note]'{zwsp}' Pipelining
* [icon-note]'{zwsp}' Reduce GC Pressure
* [icon-note]'{zwsp}' Writing gracefully
* [icon-note]'{zwsp}' Buffers best-practises
* [icon-note]'{zwsp}' EventLoop


====

[.topic.source]
== No Pipelining Optimization 

[source,java]
----
public class HttpHandler extends SimpleChannelInboundHandler<HttpRequest> {
  @Override
  public void channelRead(ChannelHandlerContext ctx, HttpRequest req) {
    ChannelFuture future = ctx.writeAndFlush(createResponse(req)); <1>
    if (!isKeepAlive(req)) {
      future.addListener(ChannelFutureListener.CLOSE); <2>
    }
  }
}
----
<1> __Write__ to the Channel and __flush__ out to the Socket.
<2> After written __close__ Socket

[.topic.source]
== Pipelining to safe syscalls! 

[source,java]
----
public class HttpPipeliningHandler extends SimpleChannelInboundHandler<HttpRequest> {
  @Override
  public void channelRead(ChannelHandlerContext ctx, HttpRequest req) {
    ChannelFuture future = ctx.writeAnd(createResponse(req)); <1>
    if (!isKeepAlive(req)) {
      future.addListener(ChannelFutureListener.CLOSE); <2>
    }
  }
  @Override
  public void channelReadComplete(ChannelHandlerContext ctx) {
    ctx.flush(); <3>
  }
}
----
<1> __Write__ to the `Channel` (__No syscall!__) but not flush yet
<2> After written __close__ socket
<3> __Flush__ out to the socket.

[.topic.source]
== write(msg) , flush() and writeAndFlush(msg)

__write(msg)__ => pass through pipeline

__flush()__ => gathering write of previous written msgs

__writeAndFlush()__ => short-cut for __write(msg)__ and __flush()__


TIP: Limit flushes as much as possible as syscalls are quite expensive.

TIP: But also limit `write(...)` as much as possible as it need to traverse the whole pipeline.


[.topic.source]
== GC-Pressure - Run Collector, run...
image::gc_pressure.jpg[width=400, align="center"]
....
http://25.media.tumblr.com/tumblr_me2eq0PnBx1rtu0cpo1_1280.jpg
....

[.topic.source]
== GC-Pressure - Reduce object creation by using VoidPromise!

Use `VoidChannelPromise` if possible

CAUTION: `Channel.write(msg)`

TIP: `Channel.write(msg, Channel.voidPromise())`

IMPORTANT: => Only do this if you are not interested in the `ChannelFuture`, and no `ChannelOutboundHandler` needs to add `ChannelFutureListener` to it!

[.topic.source]
== May write too fast!

[source,java]
----
public class BlindlyWriteHandler extends ChannelInboundHandlerAdapter {
  @Override
  public void channelActive(ChannelHandlerContext ctx) throws Exception {
    while(needsToWrite) { <1>
        ctx.writeAndFlush(createMessage()); 
    }
  }
}
----
<1> Writes till `needsToWrite` returns `false`.

CAUTION: Risk of __OutOfMemoryError__ if writing too fast and having slow receiver! 

[.topic.source]
== Correctly write with respect to slow receivers

[source,java]
----
public class GracefulWriteHandler extends ChannelInboundHandlerAdapter {
  @Override
  public void channelActive(ChannelHandlerContext ctx) {
    writeIfPossible(ctx.channel());
  }
  @Override
  public void channelWritabilityChanged(ChannelHandlerContext ctx) {
    writeIfPossible(ctx.channel());
  }

  private void writeIfPossible(Channel channel) {
    while(needsToWrite && channel.isWritable()) { <1>
      channel.writeAndFlush(createMessage()); 
    }
  }
}
----

<1> Make proper use of `Channel.isWritable()` to prevent __OutOfMemoryError__

[.topic.source]
== Configure high and low write watermarks
TIP: Set sane __WRITE_BUFFER_HIGH_WATER_MARK__ and __WRITE_BUFFER_LOW_WATER_MARK__

[source,java]
.+Server+
----
ServerBootstrap bootstrap = new ServerBootstrap();
bootstrap.childOption(ChannelOption.WRITE_BUFFER_HIGH_WATER_MARK, 32 * 1024);
bootstrap.childOption(ChannelOption.WRITE_BUFFER_LOW_WATER_MARK, 8 * 1024);
----

[source,java]
.+Client+
----
Bootstrap bootstrap = new Bootstrap();
bootstrap.option(ChannelOption.WRITE_BUFFER_HIGH_WATER_MARK, 32 * 1024);
bootstrap.option(ChannelOption.WRITE_BUFFER_LOW_WATER_MARK, 8 * 1024);
----

[.topic.source]
== Pass custom events through `ChannelPipeline`

[source,java]
.+Your custom events+
----
public enum CustomEvents {
  MyCustomEvent
}

public class CustomEventHandler extends ChannelInboundHandlerAdapter {
  @Override
  public void userEventTriggered(ChannelHandlerContext ctx, Object evt) {
    if (evt == MyCustomEvent) { // do something}
  }
}

ChannelPipeline pipeline = channel.pipeline();
pipeline.fireUserEventTriggered(MyCustomEvent);
----

TIP: Good fit for handshake notifications and more

[.topic.source]
== ByteToMessageDecoder vs. ReplayingDecoder

`ReplayingDecoder` allows easy writing of decoders but is slower than `ByteToMessageDecoder` as..

=====
* [icon-note]'{zwsp}' more overhead in methods
* [icon-note]'{zwsp}' needs to handle `ReplayingError`
=====

TIP: Rule of thumb => Use `ByteToMessageDecoder` if it is possible without make things too complicated

[.topic.source]
== Issues with using non pooled buffers
CAUTION: Use unpooled buffers with __caution__!

=====
* [icon-note]'{zwsp}' Allocation / Deallocation is __slow__
* [icon-note]'{zwsp}' Free up direct buffers == __PITA__!
=====
TIP: __Use__ pooled buffers!

[source,java]
----
Bootstrap bootstrap = new Bootstrap();
bootstrap.option(ChannelOption.ALLOCATOR, PooledByteBufAllocator.DEFAULT);
ServerBootstrap bootstrap = new ServerBootstrap();
bootstrap.childOption(ChannelOption.ALLOCATOR, PooledByteBufAllocator.DEFAULT);
----

[.topic.source]
== But configure PooledByteBufAllocator

image::condition_pooled.png[width=400]

CAUTION: Condition because of not enough areas

[source,java]
.+Configure the areas+
----
java -Dio.netty.allocator.numDirectArenas=... -Dio.netty.allocator.numHeapArenas=...
----

TIP: This will be not an issue anymore in next release. Thanks to https://github.com/netty/netty/pull/2284[Thread-Caches] in `PooledByteBufAllocator`.

[.topic.source]
== Use Pooling of buffers to reduce allocation / deallocation time! 

TIP: Pooling pays off for direct and heap buffers!

image::pooled_buffers.png[width=400]
....
https://blog.twitter.com/2013/netty-4-at-twitter-reduced-gc-overhead
....


[.topic.source]
== Write direct buffers... __Always__

CAUTION: OpenJDK and Oracle JDK copy otherwise to direct buffer by itself!

Only use heap buffers if need to operate on byte[]` in `ChannelOutboundHandler`! By default direct ByteBuf` will be returned by `ByteBufAllocator.buffer(...)`.

__Take this as rule of thumb__

[.topic.source]
== Find pattern in ByteBuf

[source,java]
.+SlowSearch :(+
----
ByteBuf buf = ...;
int index = -1;
for (int i = buf.readerIndex(); index == -1 && i <  buf.writerIndex(); i++) {
  if (buf.getByte(i) == '\n') {
    index = i;
  } 	
}
----

[source,java]
.+FastSearch :)+
----
ByteBuf buf = ...;
int index = buf.forEachByte(new ByteBufProcessor() {
  @Override
  public boolean process(byte value) {
    return value != '\n';
  }
});
----


[.topic.source]
== ByteBufProcessor continues...

`ByteBufProcessor` is faster because it...

=====
* [icon-note]'{zwsp}' can eliminate range checks 
* [icon-note]'{zwsp}' can be created and shared
* [icon-note]'{zwsp}' easier to inline by the JIT
=====

TIP: Use it whenever you need to find some pattern in a `ByteBuf`

[.topic.source]
== Other buffer tips

Prefer...

=====
* [icon-note]'{zwsp}' alloc() over Unpooled
* [icon-note]'{zwsp}' slice(), duplicate() over copy
* [icon-note]'{zwsp}' bulk operations over loops 
=====

[.topic.source]
== Messages with Payload? Yes please...

`ByteBuf` payload => extend `DefaultByteBufHolder`

=====
* [icon-note]'{zwsp}' reference-counting for free 
* [icon-note]'{zwsp}' release resources out-of-the-box
=====
image::thumb_up_2.jpg[width=180]
....
http://www.flickr.com/photos/za3tooor/65911648/
....

[.topic.source]
== File transfer ?
TIP: Use zero-memory-copy for efficient transfer of raw file content

[source,java]
----
Channel channel = ...;
FileChannel fc = ...;
channel.writeAndFlush(new DefaultFileRegion(fc, 0, fileLength));
----

CAUTION: This only works if you not need to modify the data on the fly. If so use `ChunkedWriteHandler` and `NioChunkedFile`.

[.topic.source]
== Never block the EventLoop!

CAUTION: `Thread.sleep()`

CAUTION: `CountDownLatch.await()` or any other blocking operation from
`java.util.concurrent`
 
CAUTION: Long-lived computationally intensive operations

CAUTION: Blocking operations that might take a while (e.g. DB query)

image::site_blocked.jpg[width=80]
....
http://www.flickr.com/photos/za3tooor/65911648/
....

[.topic.source]
== EventLoop extends ScheduledExecutorService, so use it!

TIP: Schedule and execute tasks via EventLoop this reduces the needed Threads and also makes sure it's Thread-safe


image::schedule.jpg[width=250]
....
http://www.flickr.com/photos/mbiddulph/3203780308/
....

[.topic.source]
== Re-use EventLoopGroup if you can!

[source,java]
----
Bootstrap bootstrap = new Bootstrap().group(new NioEventLoopGroup());
Bootstrap bootstrap2 = new Bootstrap().group(new NioEventLoopGroup());
----

[source,java]
.+Share EventLoopGroup between different Bootstraps+

----
EventLoopGroup group = new NioEventLoopGroup();
Bootstrap bootstrap = new Bootstrap().group(group);
Bootstrap bootstrap2 = new Bootstrap().group(group);
----

TIP: __Sharing__ the same `EventLoopGroup` allows to keep the resource usage (like Thread-usage) to a minimum.

[.topic.source]
== Proxy like application with context-switching issue

[source,java]
----
public class ProxyHandler extends ChannelInboundHandlerAdapter {
  @Override
  public void channelActive(ChannelHandlerContext ctx) { <1>
    final Channel inboundChannel = ctx.channel();
    Bootstrap b = new Bootstrap();
    b.group(new NioEventLooopGroup()); <2>
    ...
    ChannelFuture f = b.connect(remoteHost, remotePort);
    ...
  }
}
----
<1> Called once a new connection was accepted
<2> Use a new `EventLoopGroup` instance to handle the connection to the remote peer

CAUTION: Don't do this! This will tie up more resources than needed and introduce extra context-switching overhead.

[.topic.source]
== Proxy like application which reduce context-switching to minimum

[source,java]
----
public class ProxyHandler extends ChannelInboundHandlerAdapter {
  @Override
  public void channelActive(ChannelHandlerContext ctx) { <1>
    final Channel inboundChannel = ctx.channel();
    Bootstrap b = new Bootstrap();
    b.group(inboundChannel.eventLoop()); <2>
    ...
    ChannelFuture f = b.connect(remoteHost, remotePort);
    ...
  }
}
----
<1> Called once a new connection was accepted
<2> Share the same `EventLoop` between both Channels. This means all IO for both connected Channels are handled by the same Thread.

TIP: Always __share__ EventLoop in those Applications

[.topic.source]
== Combine operations when call from outside the `EventLoop`

[source,java]
.+Not recommended!+
----
channel.write(msg1);
channel.writeAndFlush(msg3);
----

[source,java]
.+Combine for the WIN!+
----
channel.eventLoop().execute(new Runnable() {
  @Override
  public void run() {
    channel.write(msg1);
    channel.writeAndFlush(msg3);
  } 
});
----

TIP: Always __combine__ operations if possible when act on the `Channel` from outside the `EventLoop` to reduce overhead of wakeups and object creation!

[.topic.source]
== Operations from inside ChannelHandler

[source,java]
----
public class YourHandler extends ChannelInboundHandlerAdapter {
  @Override
  public void channelActive(ChannelHandlerContext ctx) {
    // BAD (most of the times)
    ctx.channel().writeAndFlush(msg); <1>

    // GOOD
    ctx.writeAndFlush(msg); <2>
   }
}
----

<1> `Channel.*` methods  => the operation will start at the tail of the `ChannelPipeline`
<2> `ChannelHandlerContext.* methods =>  the operation will start from this `ChannelHandler` to flow through the `ChannelPipeline`. 

TIP: Use the shortest __path__ as possible to get the maximal performance.

[.topic.source]
== Share ChannelHandlers if stateless

[source,java]
----
@ChannelHandler.Shareable <1>
public class StatelessHandler extends ChannelInboundHandlerAdapter {
  @Override
  public void channelActive(ChannelHandlerContext ctx) {
    logger.debug("Now client from " + ctx.channel().remoteAddress().toString());
   }
}

public class MyInitializer extends ChannelInitializer<Channel> {
  private static final ChannelHandler INSTANCE = new StatelessHandler();
  @Override
  public void initChannel(Channel ch) {
    ch.pipeline().addLast(INSTANCE);
  }
}
----

<1> Annotate `ChannelHandler` that are stateless with `@ChannelHandler.Shareable` and use the same instance accross Channels to reduce GC. 

[.topic.source]
== Remove ChannelHandler once not needed anymore

[source,java]
----
public class OneTimeHandler extends ChannelInboundHandlerAdapter {
  @Override
  public void channelActive(ChannelHandlerContext ctx) {
    doOneTimeAction();
    ctx.channel().pipeline().remove(this); <1>
   }
}
----

<1> Remove `ChannelHandler` once not needed anymore.

TIP: This keeps the `ChannelPipeline` as __short__ as possible and so __eliminate overhead__ of traversing as much as possible.

[.topic.source]
== Use proper buffer type in MessageToByteEncoder

[source,java]
----
public class EncodeActsOnByteArray extends MessageToByteEncoder<YourMessage> {
  public EncodeActsOnByteArray() { super(false); } <1> 
  @Override
  public encode(ChannelHandlerContext ctx, YourMessage msg, ByteBuf out) {
    byte[] array = out.array(); <2>
    int offset = out.arrayOffset() + out.writerIndex();
    out.writeIndex(out.writerIndex() + encode(msg, array, offset)); <3>
  }
  private int encode(YourMessage msg, byte[] array, int offset, int len) { ... }
}
----

<1> Ensure __heap buffers__ are used when pass into `encode(...)` method. This way you can access the backing array directly
<2> Access the __backing array__ and also calculate offset
<3> Update __writerIndex__ to reflect written bytes

TIP: This saves extra byte copies. 

[.topic.source]
== To auto-read or not to auto-read

By default Netty will keep on reading data from the `Channel` once something is ready.

[source,java]
.+Need more fine grained control ?+
----
channel.config().setAutoRead(false); <1>
channel.read(); <2>
channel.config().setAutoRead(true); <3>
----
<1> Disable auto read == no more data will be read automatically of this `Channel`.
<2> Tell the `Channel` to do one read operation once new data is ready
<3> Enable again auto read == Netty will automatically read again

TIP: This can also be quite useful when writing proxy like applications!

[.topic.source]
== SSL - Don't use JDKs SSLEngine if performance matters

CAUTION: JDKs `SSLEngine` is just too slow and produces __too much GC__ :(

TIP: Use Twitters __OpenSSL__ based `SSLEngine`. For details how to compile check https://github.com/twitter/finagle/tree/master/finagle-native[Finagle native module].

[source,java]
----
import com.twitter.finagle.ssl.Ssl;
...

SSLEngine engine = Ssl.server("/path/to/cert.pem", "/path/to/key.pem", "/path/to/cachain.pem", null, null).self();
pipeline.addLast("ssl", new SslHandler(engine));
----

NOTE: Netty will ship an __OpenSSL__ based `SslHandler` by it own soon.

[.topic.source]
== Misc stuff...

TIP: You never know when `ChannelFuture` will be notified.

[source,java]
.+Bad+
----
public class OuterClass {
  private final HeavyObject instance = ....;
  public void write() { channel.writeAndFlush(msg).addListener(
      new ChannelFutureListener() { ...}); }
}
----

[source,java]
.+Good+
----
public class OuterClass {
  private final HeavyObject instance = ....;
  public void write() { channel.writeAndFlush(msg).addListener(new ListenerImpl()); }
  
  private static final class ListenerImpl implements ChannelFutureListener { ... }
}
----

[.topic.source]
== Native Transport for less GC and lower latency...
====
* [icon-note]'{zwsp}' Less GC as NIO
* [icon-note]'{zwsp}' Less synchronization
* [icon-note]'{zwsp}' Edge-Triggered!
* [icon-note]'{zwsp}' Supports __TCP_CORK__
====

CAUTION: Only works on Linux as epoll is supported atm. May change in the future

[.topic.source]
== Switching to native transport is easy

[source,java]
.+Using NIO transport+
----
Bootstrap bootstrap = new Bootstrap().group(new NioEventLoopGroup());
bootstrap.channel(NioSocketChannel.class);
----

[source,java]
.+Using native transport+
----
Bootstrap bootstrap = new Bootstrap().group(new EpollEventLoopGroup());
bootstrap.channel(EpollSocketChannel.class);
----


[.topic.source]
== Want to know more? 

TIP: Buy my book http://www.manning.com/maurer/[Netty in Action] and make me __RICH__.

image::maurer_cover150.jpg[width=100]
....
http://www.manning.com/maurer
....

__$ KA-CHING $__ 
[.topic.source]
== References

NOTE: Netty - http://netty.io

NOTE: Slides generated with Asciidoctor and DZSlides backend

NOTE: Original slide template - Dan Allen & Sarah White

NOTE: All pictures licensed with `Creative Commons Attribution` or +
`Creative Commons Attribution-Share Alike`

[.topic.ending, hrole="name"]
== Norman Maurer

[.footer]
[icon-twitter]'{zwsp}' @normanmaurer
